{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zal-eun/spanish-translation-model/blob/main/spanish_translation%EC%A0%84%EC%B2%98%EB%A6%AC_.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35e9a4df",
      "metadata": {
        "id": "35e9a4df"
      },
      "source": [
        "### 전처리 작업\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "69382a0f",
      "metadata": {
        "id": "69382a0f"
      },
      "source": [
        "#### 1. 단어 토큰화 (word tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c0179f86",
      "metadata": {
        "id": "c0179f86"
      },
      "source": [
        "\n",
        "토큰화 과정은 정보를 용도에 맞는 토큰 단위 별로 구분해 내는 작업이다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "63604a19",
      "metadata": {
        "id": "63604a19"
      },
      "source": [
        "* word_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b43c507a",
      "metadata": {
        "id": "b43c507a"
      },
      "source": [
        "동일한 스페인어 문장을 각각word_tokenize,  WordPunctTokenizer,  text_to_word_sequence\n",
        "방식으로 단어 기준 토큰화 과정을 거쳤다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ae674c1",
      "metadata": {
        "id": "4ae674c1"
      },
      "outputs": [],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "from tensorflow.keras.preprocessing.text import text_to_word_sequence"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9268a267",
      "metadata": {
        "id": "9268a267",
        "outputId": "40731fe4-144d-4df2-9c31-51627f90102b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 토큰화1 : ['Al', 'final', 'del', 'día', ',', 'podemos', 'aguantar', 'mucho', 'más', 'de', 'lo', 'que', 'pensamos', 'que', 'podemos']\n"
          ]
        }
      ],
      "source": [
        "print('단어 토큰화1 :',word_tokenize(\"Al final del día, podemos aguantar mucho más de lo que pensamos que podemos\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5db92dd2",
      "metadata": {
        "id": "5db92dd2",
        "outputId": "d80dd495-53cb-44e8-b2d8-3ff66f3ce6d5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 토큰화1 : ['Al', 'final', 'del', 'día', ',', 'podemos', 'aguantar', 'mucho', 'más', 'de', 'lo', 'que', 'pensamos', 'que', 'podemos']\n"
          ]
        }
      ],
      "source": [
        "print('단어 토큰화1 :', WordPunctTokenizer().tokenize(\"Al final del día, podemos aguantar mucho más de lo que pensamos que podemos\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9aeebb58",
      "metadata": {
        "id": "9aeebb58"
      },
      "source": [
        "영어 단어 토큰화 할 경우에, word_tokenize는 아포스트로피를 앞단어에 붙여서 토큰화한다. 반면, WordPunctTokenizer에서는 아포스트로피를 개별적으로 토큰화한다. 그러나 스페인어의 경우에 아포스트로피가 붙어있는 경우가 없으므로, 두 결과가 동일하게 나온다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "bc5a2c99",
      "metadata": {
        "id": "bc5a2c99",
        "outputId": "296e5e98-0a97-4609-e610-599a98eb0d78"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 토큰화1 : ['al', 'final', 'del', 'día', 'podemos', 'aguantar', 'mucho', 'más', 'de', 'lo', 'que', 'pensamos', 'que', 'podemos']\n"
          ]
        }
      ],
      "source": [
        "print('단어 토큰화1 :',text_to_word_sequence(\"Al final del día, podemos aguantar mucho más de lo que pensamos que podemos\"))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1eeb2038",
      "metadata": {
        "id": "1eeb2038"
      },
      "source": [
        "앞선 토큰화 방식과는 다르게, text_to_word_sequence(케라스)에서는 마침표, 컴마 등을 제거한다. 또한 대문자를 소문자로 바꾼다. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fc6ebfc5",
      "metadata": {
        "id": "fc6ebfc5"
      },
      "source": [
        "#### 2. 문장 토큰화(sentence tokenization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b9628000",
      "metadata": {
        "id": "b9628000"
      },
      "source": [
        "* sent_tokenize"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "019cfbb9",
      "metadata": {
        "id": "019cfbb9"
      },
      "source": [
        "1) 영어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf3d58d4",
      "metadata": {
        "id": "cf3d58d4",
        "outputId": "2ff558b3-5ef6-483e-ddf4-67532dfde243"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "영어 문장 토큰화 : ['He is looking for Ph.D. students.', 'Mr.Kim kept his word.', 'He dug a hole in the midst of some reeds.', 'He looked about, to make sure no one was near.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "\n",
        "text = \"He is looking for Ph.D. students. Mr.Kim kept his word. He dug a hole in the midst of some reeds. He looked about, to make sure no one was near.\"\n",
        "print('영어 문장 토큰화 :',sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "74dfa6d0",
      "metadata": {
        "id": "74dfa6d0"
      },
      "source": [
        "영어 토큰화의 경우에 'Ph.D.','Mr.kim'등이 개별적으로 토큰화되지 않는다는 점에서 오직 마침표가 토큰화의 기준이 아님을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "847e364b",
      "metadata": {
        "id": "847e364b"
      },
      "source": [
        "2) 스페인어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "67056b8c",
      "metadata": {
        "id": "67056b8c",
        "outputId": "9457fd4b-d3bf-41b9-e409-bec371de39a2"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "문장 토큰화:  ['Ud.', 'perdió el equilibrio y cayó.', 'Uds.', 'le dan mucho miedo.', 'No tuvo tiempo ni para dar un grito.']\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import sent_tokenize\n",
        "text = \"Ud. perdió el equilibrio y cayó. Uds. le dan mucho miedo. No tuvo tiempo ni para dar un grito.\"\n",
        "print('문장 토큰화: ', sent_tokenize(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "35565f08",
      "metadata": {
        "id": "35565f08"
      },
      "source": [
        "스페인어에서 Ud.는 usted의 줄임말로 '당신'을 뜻한다. <br>\n",
        "유사하게 Uds.는 usted의 복수형인 ustedes의 줄임말로 '당신들'을 뜻한다. <br><br>\n",
        "Nltk에서는 ud.와 uds.모두 개별 문장으로 인식하여 토큰 작업을 진행했다. <br>\n",
        "이는 영어 문장 토큰화에 비해 스페인어 인지력이 낮은 것으로 볼 수 있다. (따라서, 이후 작업에서는 여러 제약이 따른다는 점에서 nltk보다는 spacy를 주로 사용했다. )"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "509d4378",
      "metadata": {
        "id": "509d4378"
      },
      "source": [
        "3) 한국어"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "03b1370f",
      "metadata": {
        "id": "03b1370f",
        "outputId": "6578f222-7282-4fee-c8c0-57dfcaff526b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "한국어 문장 토큰화 : ['그 피곤한 사람은 잠이 확깼다.', '그는 중심을 잃고 떨어졌다.', '소리지를 새도 없었다.', '잠이 덜 깼고, 돌 몇 개를 움켜쥘 뿐이었다.', '그는 수면위로 올라가기 위해 우물 안에서 고군분투했다.', '그의 손가락은 긴장해서 거의 아무것도 잡을 수 없었다.']\n"
          ]
        }
      ],
      "source": [
        "import kss\n",
        "\n",
        "text = '그 피곤한 사람은 잠이 확깼다. 그는 중심을 잃고 떨어졌다. 소리지를 새도 없었다. 잠이 덜 깼고, 돌 몇 개를 움켜쥘 뿐이었다. 그는 수면위로 올라가기 위해 우물 안에서 고군분투했다. 그의 손가락은 긴장해서 거의 아무것도 잡을 수 없었다.'\n",
        "print('한국어 문장 토큰화 :',kss.split_sentences(text))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d248758",
      "metadata": {
        "id": "6d248758"
      },
      "source": [
        "#### 3. 품사 태깅(Part-of-speech tagging)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dcd816c7",
      "metadata": {
        "id": "dcd816c7"
      },
      "source": [
        "1) nltk"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4ce079cd",
      "metadata": {
        "id": "4ce079cd",
        "outputId": "8f1fa4ac-4c9b-4ce0-8a10-0af99afea9a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "단어 토큰화 : ['el', 'ruiseñor', 'voló', 'al', 'rosal', 'que', 'crecía', 'en', 'torno', 'del', 'viejo', 'reloj', 'de', 'sol', '.']\n",
            "품사 태깅 : [('el', 'NN'), ('ruiseñor', 'NN'), ('voló', 'NN'), ('al', 'NN'), ('rosal', 'NN'), ('que', 'NN'), ('crecía', 'NN'), ('en', 'IN'), ('torno', 'NN'), ('del', 'NN'), ('viejo', 'NN'), ('reloj', 'NN'), ('de', 'IN'), ('sol', 'NN'), ('.', '.')]\n"
          ]
        }
      ],
      "source": [
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.tag import pos_tag\n",
        "\n",
        "text = \"el ruiseñor voló al rosal que crecía en torno del viejo reloj de sol.\"\n",
        "tokenized_sentence = word_tokenize(text)\n",
        "\n",
        "print('단어 토큰화 :',tokenized_sentence)\n",
        "print('품사 태깅 :',pos_tag(tokenized_sentence))"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d89a4d0b",
      "metadata": {
        "id": "d89a4d0b"
      },
      "source": [
        "nltk로 단어 토큰화 결과 명확하게 품사 정렬이 되지 않는 모습을 알 수 있다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2a10d687",
      "metadata": {
        "id": "2a10d687"
      },
      "source": [
        "2) spacy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9a9658de",
      "metadata": {
        "id": "9a9658de",
        "outputId": "99ee65d6-9b10-4ad5-9ae6-5825abec92f3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[('el', 'DET'), ('ruiseñor', 'NOUN'), ('voló', 'VERB'), ('al', 'ADP'), ('rosal', 'NOUN'), ('que', 'PRON'), ('crecía', 'VERB'), ('en', 'ADP'), ('torno', 'NOUN'), ('del', 'DET'), ('viejo', 'ADJ'), ('reloj', 'NOUN'), ('de', 'ADP'), ('sol', 'NOUN'), ('.', 'PUNCT')]\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "doc = nlp(\"el ruiseñor voló al rosal que crecía en torno del viejo reloj de sol.\")\n",
        "print([(w.text, w.pos_) for w in doc])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "da3622f7",
      "metadata": {
        "id": "da3622f7"
      },
      "source": [
        "spacy에서 품사 분류는 용어에 따라 <br>\n",
        "DET : 한정사, NOUN : 명사, VERB : 동사, ADP :전치사, PRON : 관계대명사, ADJ :부사, PUNCT :구두점\n",
        "등으로 나타낸다. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "84bf07b3",
      "metadata": {
        "id": "84bf07b3"
      },
      "source": [
        "### 2.2 텍스트 전처리"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0bfbe4f5",
      "metadata": {
        "id": "0bfbe4f5"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import shutil\n",
        "import zipfile\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "import unicodedata\n",
        "import urllib3"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d867a6fc",
      "metadata": {
        "id": "d867a6fc"
      },
      "source": [
        "#### 1. 표제어 추출(Lemmatization)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "418515d8",
      "metadata": {
        "id": "418515d8"
      },
      "source": [
        "pip install spacy_spanish_lemmatizer"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2cf93ed6",
      "metadata": {
        "id": "2cf93ed6"
      },
      "source": [
        "nltk에서 스페인어를 적극 활용하기 어렵다고 판단, spacy를 이용한다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0d3f93f6",
      "metadata": {
        "id": "0d3f93f6",
        "outputId": "fa574dd4-b417-4ec2-9f2a-d080c7fce2f9"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'haces'"
            ]
          },
          "execution_count": 48,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "#EX\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "\n",
        "\n",
        "spanish_lemmatizer = WordNetLemmatizer()\n",
        "spanish_lemmatizer.lemmatize('haces','v')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "848e5605",
      "metadata": {
        "id": "848e5605"
      },
      "source": [
        "haces는 'hacer'라는 동사의 2인칭 변화형이다. 그러나 lemmatizer가 해당 str이 동사임을 명시해줬음에도, 동사 원형을 산출하지 못한다."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e0d6521d",
      "metadata": {
        "id": "e0d6521d"
      },
      "source": [
        "* spacy를 통해서 동사 원형을 산출해낼 수 있다. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04ab8397",
      "metadata": {
        "id": "04ab8397",
        "outputId": "e6064449-9c46-4c6b-d4c2-55a9583063a5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "el --> el\n",
            "ruiseñor --> ruiseñor\n",
            "voló --> volar\n",
            "al --> al\n",
            "rosal --> rosal\n",
            "que --> que\n",
            "crecía --> crecíar\n",
            "en --> en\n",
            "torno --> torno\n",
            "del --> del\n",
            "viejo --> viejo\n",
            "reloj --> reloj\n",
            "de --> de\n",
            "sol --> sol\n",
            ". --> .\n"
          ]
        }
      ],
      "source": [
        "import spacy\n",
        "nlp = spacy.load(\"es_core_news_sm\")\n",
        "\n",
        "doc =nlp(\"el ruiseñor voló al rosal que crecía en torno del viejo reloj de sol.\")\n",
        "for token in doc:\n",
        "    print(token.text, '-->', token.lemma_)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5f136df4",
      "metadata": {
        "id": "5f136df4"
      },
      "source": [
        "#### 2. 어간추출(stemming)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c94a07c3",
      "metadata": {
        "id": "c94a07c3"
      },
      "outputs": [],
      "source": [
        "*아직 spacy에는 어간 추출 기능이 구현되어 있지 않다. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce39b0fd",
      "metadata": {
        "id": "ce39b0fd"
      },
      "source": [
        "### 3. 번역기"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ba77982",
      "metadata": {
        "id": "7ba77982"
      },
      "source": [
        "스페인 언어의 영어와의 문자적 차별성을 기반으로 전처리 코드를 작성한다. "
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b396a661",
      "metadata": {
        "id": "b396a661"
      },
      "source": [
        "* 악센트 제거 / 에녜(ñ) 제거"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d6640adb",
      "metadata": {
        "id": "d6640adb"
      },
      "outputs": [],
      "source": [
        "\n",
        "def to_ascii(s):\n",
        "    return ''.join(c for c in unicodedata.normalize('NFD',s)\n",
        "                  if unicodedata.category(c) !='Mn')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c6d264a2",
      "metadata": {
        "id": "c6d264a2",
        "outputId": "fdf7a38d-c33b-4db8-86e3-138f377e3f3e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "los árboles --> los arboles\n",
            "mañana --> manana\n"
          ]
        }
      ],
      "source": [
        "#Ex\n",
        "example1 ='los árboles'\n",
        "example2= 'mañana'\n",
        "print(example1, '-->', to_ascii(example1))\n",
        "print(example2, '-->', to_ascii(example2))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6e2b033d",
      "metadata": {
        "id": "6e2b033d"
      },
      "source": [
        "* 특수 기호 제거 (¿,¡)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97d8d729",
      "metadata": {
        "id": "97d8d729"
      },
      "outputs": [],
      "source": [
        "\n",
        "def preprocess_sentence(sent):\n",
        "    sent = to_ascii(sent.lower())\n",
        "    sent = re.sub(r\"([?.!,¿])\", r\" \\1\", sent)\n",
        "    sent = re.sub(r\"[^a-zA-Z!.?¿¡]+\", r\" \", sent)\n",
        "    sent = re.sub(r\"\\s+\", \" \", sent)\n",
        "    return sent"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7364785a",
      "metadata": {
        "id": "7364785a",
        "outputId": "5c5a7fcf-7276-4b99-d3a3-30fe3aa7e56a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "전처리 전 스페인어 문장 : ¡mañana, el árbol será más grande que hoy!\n",
            "전처리 후 스페인어 문장 : ¡manana el arbol sera mas grande que hoy !\n"
          ]
        }
      ],
      "source": [
        "#Ex\n",
        "spa_sent = u'¡mañana, el árbol será más grande que hoy!'\n",
        "print('전처리 전 스페인어 문장 :', spa_sent)\n",
        "print('전처리 후 스페인어 문장 :', preprocess_sentence(spa_sent))\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}